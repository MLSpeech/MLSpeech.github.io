<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>{Deep} Phonetic Tools</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/modern-business.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

   <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html">{Deep} Phonetic Tools</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="active">
                        <a href="about.html">About</a>
                    </li>
                    <li>
                        <a href="get_started.html">Get Started</a>
                    </li>
                    <li>
                        <a href="faq.html">FAQ</a>
                    </li>
                    <li>
                        <a href="tutorial.html">Tutorial</a>
                    </li>
                    <li>
                        <a href="who_we_are.html">Who We Are</a>
                    </li>
                    <li>
                        <a href="github.html">GitHub</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

        <!-- Page Heading/Breadcrumbs -->
        <div class="row">
            <div class="col-lg-12">
                <h1 class="page-header">Available Tools
                </h1>
                <ol class="breadcrumb">
                    <li><a href="index.html">Home</a>
                    </li>
                    <li class="active">About</li>
                </ol>
            </div>
        </div>
        <!-- /.row -->

        <!-- Service Tabs -->
        <div class="row">
            <div class="col-lg-12">

                <ul id="myTab" class="nav nav-tabs nav-justified">
                    <li class="active"><a href="#service-one" data-toggle="tab"> Word Duration</a>
                    </li>
                    <li class=""><a href="#service-two" data-toggle="tab">Vowel Duration</a>
                    </li>
                    <li class=""><a href="#service-three" data-toggle="tab">Voice Onset Time</a>
                    </li>
                    <li class=""><a href="#service-four" data-toggle="tab">Formant Estimation &amp; Tracking</a>
                    </li>
                </ul>

                <div id="myTabContent" class="tab-content">
                    <div class="tab-pane fade active in" id="service-one">
                        <h4>Word Duration</h4>
                        <p>DeepWDM generates automatic measurements of word durations. Input to the system is a sound file of an arbitrary length with a single word. The system outputs a textgrid with the duration of the word. The tool has three RNN models available for use for this taks. The trained model does not require phonetic or orthographic transcription o the words as input. </p>
                       <br><br>
                            
                        <center><figure>
                            <img src="DeepWDM_peach_male.jpg" alt="DeepWDM" width="80%" margin="auto" display="block">
                            <figcaption>DeepWDM: the word <i>peach</i> pronunounced by a male.</figcaption>
                        </figure></center>
                    </div>
                    <div class="tab-pane fade" id="service-two">
                        <h4>AutoVowelDuration</h4>
                        <p>AutoVowelDuration generates automatic measurements of vowel durations. The system takes as input an arbitrary length segment of an acoustic signal containing a single vowel that is preceded and followed by consonants, and outputs the duration of the vowel. A DNN-based algorithm (Adi et al., 2015) and structured prediction-based algorithm (Adi et al., 2016) were designed for this task. Critically, the trained model can automatically estimate vowel durations without phonetic or orthographic transcription.</p><br>
                        <p>The vowel duration annotator has been compared to the current state-of-the-art alignment system for segment-level annotation: the FAVE forced aligner (Rosenfelder et al. 2011), based on the Penn Phonetics Lab Forced Aligner (Yuan &amp; Liberman, 2008). Our automatic annotation system performs at a high degree of accuracy: it matches the vowel boundaries selected by manual annotation, resulting in a closer match to these manual annotations than the FAVE aligner; and recovers the same parameters of a statistical model based on manual measurements (Adi et al., 2015; Adi et al., 2016).</p>
                        
                        <small>
                            Adi, Y., Keshet, J., &amp; Goldrick, M. (2015). Vowel duration measurement using deep neural networks, IEEE 25th International
                            Workshop on Machine Learning for Signal Processing (MLSP)<br>

                            Adi, Y., Keshet, J., Cibelli, E., Gustafson, E., Clopper, C., and Goldrick, M. (2016). Automatic measurement of vowel duration via structured prediction. Manuscript submitted for publication<br>

                            Yuan. J. &amp; Liberman, M. (2008). Speaker identification on the SCOTUS corpus. Journal of the Acoustical Society of America, 123(5):3878, 2008<br>

                            Rosenfelder, I., Fruehwald, J., Evanini, K., &amp; Jiahong Y. (2011). FAVE (Forced Alignment and Vowel Extraction) Program Suite. <a href=http://fave.ling.upenn.edu>http://fave.ling.upenn.edu</a><br>
                        </small>
                        <br><br>
                            
                        <center><figure>
                            <img src="AutoVowelDuration_goose.jpg" alt="AutoVowelDuration" width="80%" margin="auto" display="block">
                            <figcaption>AutoVowelDuraiton: the word <i>goose</i> pronunounced by a male.</figcaption>
                        </figure></center>

 
                    </div>
                    <div class="tab-pane fade" id="service-three">
                        <h4>AutoVOT and DeepVOT</h4>
                        <p>Many studies are concerned with measurement of voice onset time (VOT) of stop consonants. While most of the work on automatic measurement of VOT has focused on positive VOT, common in American English stops, in many languages VOT can be negative, reflecting a period of voicing prior to the release of the stop burst. In this tool we try to address the problem of accurate estimation of both positive and negative VOTs. The input to the algorithm is a speech segment of an arbitrary length containing a single word-initial stop consonant, and the output is the burst onset, the onset of the voicing of the following vowel, and the time of the prevoicing onset (if prevoicing is present). Manually labeled data was used to train a recurrent neural network that can model the dynamic temporal behavior of the input signal, and outputs the events’ onsets (and duration of VOT).</p>
                        <p>Comparisons to manually-annotated data show that the VOT system is able to meet or exceed performance of most existing systems (Henry, Sonderegger, &amp; Keshet, 2012). Because the VOT system is capable of flexible input, it is robust in measurements where variable speaking rates, VOT durations, or speech errors are anticipated (Goldrick et al., 2016).</p>
                       <br><br>

                        <center><figure>
                            <img src="AutoVOT_peach_male.jpg" alt="AutoVOT" width="80%" margin="auto" display="block">
                            <figcaption>AutoVOT: the word <i>peach</i> pronunounced by a male.</figcaption>
                        </figure></center>

                    </div>
                    <div class="tab-pane fade" id="service-four">
                        <h4>DeepFormants</h4>
                        <p>Formant frequency estimation and tracking are among the most fundamental problems in speech processing. Speech analyses require the accurate estimation of formants from a stationary point in time (e.g. the midpoint of a vowel), as well as dynamic tracking of frequencies across a signal. Traditionally, formant estimation and tracking is done using ad-hoc signal processing methods. DeepFormants (Dissen &amp; Keshet, 2016) is based on machine learning techniques trained on an annotated corpus of read speech (Deng et al., 2006). Our acoustic signal representation is composed of LPC-based cepstral coefficients with a range of model orders (number of LPC poles) and pitch-synchronous cepstral coefficients. Two deep network architectures are used as learning algorithms: a deep feed-forward network for the estimation task and a recurrent neural network for the tracking task. The performance of our methods compares favorably with mainstream LPC-based implementations and state-of-the-art tracking algorithms. Preliminary results suggest that, like existing systems such as DARLA and FAVE (Reddy &amp; Stanford, 2015), our system can capture dialect-level differences in the tense-lax distinction of front vowels that differ between speakers of Northern and Southern varieties of American English.</p><br>

                        
                        <center><table class="w3-table-all" style="width:40%" border=1 frame=void rules=rows>
                            <tr>
                                <th></th>
                                <th>Method</u></th>
                                <th>F1 [Hz]</th>
                                <th>F2 [Hz]</th>      
                                <th>F3 [Hz]</th>
                            </tr>
                            <tr><td rowspan="2">mean</td>
                                <td>DeepFormant</td>
                                <td><b>48</b></td>
                                <td><b>83</b></td>
                                <td><b>100</b></td></tr>
                                <tr><td>Praat</td>
                                <td>75</td>
                                <td>115</td>
                                <td>151</td>
                            </tr>
                            <tr><td rowspan="2">median</td>
                                <td>DeepFormant</td>
                                <td><b>38</b></td>
                                <td><b>62</b></td>
                                <td><b>75</b></td></tr>
                                <tr><td>Praat</td>
                                <td>48</td>
                                <td>78</td>
                                <td>88</td>
                            </tr>
                            <tr><td rowspan="2">max</td>
                                <td>DeepFormant</td>
                                <td><b>528</b></td>
                                <td><b>716</b></td>
                                <td><b>1509</b></td></tr>
                                <tr><td>Praat</td>
                                <td>1611</td>
                                <td>1711</td>
                                <td>1633</td>
                            </tr>
                        </table><br>Error in estimation of formant frequencies of whole vowels using DeepFormants and Praat<br></center>
                        <br><br>

                        <small>
                            Yehoshua Dissen and Joseph Keshet, <a href="http://u.cs.biu.ac.il/~jkeshet/papers/DissenKe16.pdf">Formant Estimation and Tracking using Deep Learning</a>, The 17th Annual Conference of the International Speech Communication Association (Interspeech), San Francisco, CA, 2016.<br>

                            L. Deng, X. Cui, R. Pruvenok, Y. Chen, S. Momen, and A. Alwan, "A database of vocal tract resonance trajectories for research in speech processing," in Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, vol. 1. IEEE, 2006, pp. I–I.<br>
                            
                            S. Reddy and J. N. Stanford, "Toward completely automated vowel extraction: Introducing darla," Linguistics Vanguard, 2015.<br>
                        </small>
                        <br><br>

                    </div>
                </div>

            </div>
        </div>

 

        <hr>

        <!-- Footer -->
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; {Deep} Phonetic Tools 2016</p>
                </div>
            </div>
        </footer>

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
